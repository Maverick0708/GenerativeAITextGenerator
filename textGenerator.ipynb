{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The below code is to train an LSTM model to generate new text. The application has effectively 2 parts, encoder and decoder. I use a bidirectional LSTM and a unidirectional LSTM for encoder and 2 dense layers as decoder."
      ],
      "metadata": {
        "id": "JgsR4NQqhdpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a tokenized corpus\n",
        "\n",
        "The first step is too read a textfile for creating a corpus. A corpus is a large and structured set of texts used for natural language processing (NLP) tasks like text mining, sentiment analysis, machine translation,etc. I will use this corpus to train my model. The FitOnTexts function of tokenizer is used  to generate a word index dictionary that maps each unique word in the provided texts to a unique integer. This mapping will be used to convert string input into numerical format to be used as input to machine learning models. Machine Learning models only understands numbers"
      ],
      "metadata": {
        "id": "5Q30oKvPiW0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tWUL6VJrhbAx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "text = open(\"employee.txt\", encoding=\"utf-8\").read()\n",
        "corpus = text.lower().splitlines()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "wordDict = len(tokenizer.word_index)+1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating n-gram sequences\n",
        "\n",
        "We need to convert the text input into numericals.TextToSequences is used for this purpose. It converts list of strings into a list of sequences/integers. Each word in the texts is replaced by its corresponding integer index as determined by the fit_on_texts method. Next we make input sequences. Each line needs to be converted to n-gram sequences. Last word in each n-gram sequence will be treated as label and all the words preceding it as inputs. For example, The sentence 'Shivam loves building large language models' should be converted to [\"Shivam\", \"Shivam loves\", \"Shivam loves building\",\"Shivam loves building large\",\"Shivam loves building large language\",\"Shivam loves building large language models\"]. Though for training purposes, we will ignore n-gram sequences with length 1 as they can't have both input and label"
      ],
      "metadata": {
        "id": "lZZe_HK-k8yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngramSequencesList = []\n",
        "for line in corpus:\n",
        "    listOfTokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    #print(token_list)\n",
        "    for i in range(1, len(listOfTokens)):\n",
        "        ngramSequence = listOfTokens[:i+1]\n",
        "        ngramSequencesList.append(ngramSequence)\n",
        "sequenceMaxLength = max([len(i) for i in ngramSequencesList])\n"
      ],
      "metadata": {
        "id": "Xl8ARco9k9Ow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "\n",
        "The n-fram sequences to be used as input must be of same size, so padding is done"
      ],
      "metadata": {
        "id": "plcaLa1wo46w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "ngramSequencesList = np.array(pad_sequences(ngramSequencesList, maxlen=sequenceMaxLength, padding='pre'))"
      ],
      "metadata": {
        "id": "1GCA5zeKo5SN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperating input from labels and one hot encoding\n",
        "\n",
        "The input and labels need to be seperated for everu n-gram sequence. Labels need to be one hot encoded to remove any relationship between the labels"
      ],
      "metadata": {
        "id": "GTovCs0vpRvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.utils as keras_utils\n",
        "\n",
        "input, label = ngramSequencesList[:,:-1],ngramSequencesList[:,-1]\n",
        "label = keras_utils.to_categorical(label, num_classes=wordDict)"
      ],
      "metadata": {
        "id": "43htE1KwpSQO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model\n",
        "\n",
        "Model contains 2 parts, encoder and decoder\n",
        "\n",
        "## Encoder\n",
        "\n",
        "Encoder consists of an Embedding layer which converts each individual token(represented in numerical form) into dense embedded vectors, each dimensions size mentioned in second parameter(in this case 100). This ensures that the semantics and contexts of the words is captured and the size of the token does not grow quadratically with the size of the corpus.\n",
        "\n",
        " A bidirectional LSTM layers is added. LSTM input shape must be compatable with embedding outsput shape. Since LSTM is not the first layer, keras will take care of this implicitly and we don't need to mention input shape explicitly. return_sequence=True is added to return the full sequence of outputs for another LSTM stack\n",
        "\n",
        " A dropout layer is added to avoid overfitting\n",
        "\n",
        " a unidirectional LSTM is added as the last layer. The output is feeded into the decoder\n",
        "\n",
        "# Decoder\n",
        "\n",
        "Decoder has 2 dense layer. First dense layer is used to recieve input from encoder and relu function is applied. Regulizer is used to avoid overfitting\n",
        "\n",
        "Last sende layer is used as the output layer for the predicted word. Softmax is used in this layer to chose the word with the highest probability\n",
        "\n",
        "Optimizer used is ADAM"
      ],
      "metadata": {
        "id": "zRVVH9cOqlD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#------------------Encoder------------------------------------\n",
        "model.add(Embedding(wordDict, 100, input_length=sequenceMaxLength-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "#model.add(LSTM(150))\n",
        "#-------------------Decoder----------------------------------------------\n",
        "model.add(Dense(wordDict, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Dense(wordDict, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "mKvtX_VRqnNY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "Here the model is simply trained"
      ],
      "metadata": {
        "id": "Ro3TBm0ivjuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(input, label, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD3g_EI-viYe",
        "outputId": "2f3a8b06-8f74-4941-e914-d365943c958d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "130/130 [==============================] - 96s 672ms/step - loss: 5.8491 - accuracy: 0.0361\n",
            "Epoch 2/50\n",
            "130/130 [==============================] - 89s 677ms/step - loss: 5.5669 - accuracy: 0.0448\n",
            "Epoch 3/50\n",
            "130/130 [==============================] - 89s 683ms/step - loss: 5.4832 - accuracy: 0.0542\n",
            "Epoch 4/50\n",
            "130/130 [==============================] - 88s 675ms/step - loss: 5.3314 - accuracy: 0.0684\n",
            "Epoch 5/50\n",
            "130/130 [==============================] - 90s 689ms/step - loss: 5.1176 - accuracy: 0.0838\n",
            "Epoch 6/50\n",
            "130/130 [==============================] - 88s 681ms/step - loss: 4.9081 - accuracy: 0.0992\n",
            "Epoch 7/50\n",
            "130/130 [==============================] - 91s 698ms/step - loss: 4.7131 - accuracy: 0.1148\n",
            "Epoch 8/50\n",
            "130/130 [==============================] - 86s 664ms/step - loss: 4.4930 - accuracy: 0.1444\n",
            "Epoch 9/50\n",
            "130/130 [==============================] - 89s 684ms/step - loss: 4.2828 - accuracy: 0.1608\n",
            "Epoch 10/50\n",
            "130/130 [==============================] - 87s 673ms/step - loss: 4.0613 - accuracy: 0.1771\n",
            "Epoch 11/50\n",
            "130/130 [==============================] - 87s 669ms/step - loss: 3.8251 - accuracy: 0.2063\n",
            "Epoch 12/50\n",
            "130/130 [==============================] - 87s 667ms/step - loss: 3.6080 - accuracy: 0.2265\n",
            "Epoch 13/50\n",
            "130/130 [==============================] - 88s 676ms/step - loss: 3.3699 - accuracy: 0.2505\n",
            "Epoch 14/50\n",
            "130/130 [==============================] - 90s 694ms/step - loss: 3.1212 - accuracy: 0.2768\n",
            "Epoch 15/50\n",
            "130/130 [==============================] - 88s 675ms/step - loss: 2.8678 - accuracy: 0.3042\n",
            "Epoch 16/50\n",
            "130/130 [==============================] - 86s 665ms/step - loss: 2.6381 - accuracy: 0.3434\n",
            "Epoch 17/50\n",
            "130/130 [==============================] - 90s 692ms/step - loss: 2.3943 - accuracy: 0.3841\n",
            "Epoch 18/50\n",
            "130/130 [==============================] - 88s 680ms/step - loss: 2.2001 - accuracy: 0.4301\n",
            "Epoch 19/50\n",
            "130/130 [==============================] - 89s 686ms/step - loss: 1.9518 - accuracy: 0.4727\n",
            "Epoch 20/50\n",
            "130/130 [==============================] - 86s 663ms/step - loss: 1.6823 - accuracy: 0.5410\n",
            "Epoch 21/50\n",
            "130/130 [==============================] - 88s 675ms/step - loss: 1.4514 - accuracy: 0.6058\n",
            "Epoch 22/50\n",
            "130/130 [==============================] - 89s 689ms/step - loss: 1.2820 - accuracy: 0.6628\n",
            "Epoch 23/50\n",
            "130/130 [==============================] - 87s 673ms/step - loss: 1.0918 - accuracy: 0.7227\n",
            "Epoch 24/50\n",
            "130/130 [==============================] - 86s 666ms/step - loss: 0.9349 - accuracy: 0.7656\n",
            "Epoch 25/50\n",
            "130/130 [==============================] - 88s 671ms/step - loss: 0.8042 - accuracy: 0.8043\n",
            "Epoch 26/50\n",
            "130/130 [==============================] - 86s 665ms/step - loss: 0.6843 - accuracy: 0.8421\n",
            "Epoch 27/50\n",
            "130/130 [==============================] - 90s 692ms/step - loss: 0.5922 - accuracy: 0.8696\n",
            "Epoch 28/50\n",
            "130/130 [==============================] - 90s 695ms/step - loss: 0.5264 - accuracy: 0.8893\n",
            "Epoch 29/50\n",
            "130/130 [==============================] - 88s 678ms/step - loss: 0.4595 - accuracy: 0.9117\n",
            "Epoch 30/50\n",
            "130/130 [==============================] - 88s 674ms/step - loss: 0.4242 - accuracy: 0.9232\n",
            "Epoch 31/50\n",
            "130/130 [==============================] - 88s 680ms/step - loss: 0.4388 - accuracy: 0.9057\n",
            "Epoch 32/50\n",
            "130/130 [==============================] - 90s 693ms/step - loss: 0.3808 - accuracy: 0.9276\n",
            "Epoch 33/50\n",
            "130/130 [==============================] - 88s 676ms/step - loss: 0.3202 - accuracy: 0.9471\n",
            "Epoch 34/50\n",
            "130/130 [==============================] - 88s 674ms/step - loss: 0.2696 - accuracy: 0.9617\n",
            "Epoch 35/50\n",
            "130/130 [==============================] - 89s 686ms/step - loss: 0.2385 - accuracy: 0.9668\n",
            "Epoch 36/50\n",
            "130/130 [==============================] - 88s 674ms/step - loss: 0.2167 - accuracy: 0.9733\n",
            "Epoch 37/50\n",
            "130/130 [==============================] - 91s 701ms/step - loss: 0.2089 - accuracy: 0.9747\n",
            "Epoch 38/50\n",
            "130/130 [==============================] - 88s 680ms/step - loss: 0.2117 - accuracy: 0.9718\n",
            "Epoch 39/50\n",
            "130/130 [==============================] - 88s 680ms/step - loss: 0.2755 - accuracy: 0.9557\n",
            "Epoch 40/50\n",
            "130/130 [==============================] - 88s 678ms/step - loss: 0.5875 - accuracy: 0.8549\n",
            "Epoch 41/50\n",
            "130/130 [==============================] - 88s 675ms/step - loss: 0.5002 - accuracy: 0.8833\n",
            "Epoch 42/50\n",
            "130/130 [==============================] - 90s 691ms/step - loss: 0.3677 - accuracy: 0.9244\n",
            "Epoch 43/50\n",
            "130/130 [==============================] - 88s 680ms/step - loss: 0.2732 - accuracy: 0.9572\n",
            "Epoch 44/50\n",
            "130/130 [==============================] - 88s 681ms/step - loss: 0.2328 - accuracy: 0.9699\n",
            "Epoch 45/50\n",
            "130/130 [==============================] - 90s 691ms/step - loss: 0.1909 - accuracy: 0.9795\n",
            "Epoch 46/50\n",
            "130/130 [==============================] - 89s 688ms/step - loss: 0.1702 - accuracy: 0.9819\n",
            "Epoch 47/50\n",
            "130/130 [==============================] - 90s 687ms/step - loss: 0.1605 - accuracy: 0.9836\n",
            "Epoch 48/50\n",
            "130/130 [==============================] - 90s 685ms/step - loss: 0.1531 - accuracy: 0.9844\n",
            "Epoch 49/50\n",
            "130/130 [==============================] - 87s 673ms/step - loss: 0.1527 - accuracy: 0.9846\n",
            "Epoch 50/50\n",
            "130/130 [==============================] - 88s 678ms/step - loss: 0.1458 - accuracy: 0.9865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "A prompt is used for the model to use as input for predicting next words. I predict upto 100 words here(Prompt size + 100 words)"
      ],
      "metadata": {
        "id": "SAPxu6CTviHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me Something about employee training\"\n",
        "prectionWodCOunt = 100\n",
        "\n",
        "for _ in range(prectionWodCOunt):\n",
        "    listOfTokens = tokenizer.texts_to_sequences([prompt])[0]\n",
        "    listOfTokens = pad_sequences([listOfTokens], maxlen=sequenceMaxLength-1, padding='pre')\n",
        "    predictedProbabilities = model.predict(listOfTokens, verbose=0)\n",
        "    predictedClass = np.argmax(predictedProbabilities)\n",
        "    outWord = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predictedClass:\n",
        "            outWord = word\n",
        "            break\n",
        "    prompt += \" \" + outWord\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt0fTWAxybi6",
        "outputId": "0605f1d0-cb9b-4b2a-ff1a-a4f33d47b9e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me Something about employee training also write as a job due to lack of sufficient support during induction develop employees have little or no stress their efficiency is enhanced such employees can make high quality products they can avoid cases of accidents in the market it is out that conflicts are also confident that they perform their they are trained on how to handle various publics of the organization employees who are trained on communication are able to carry out audience analysis before packaging communication materials for them with modern skills to productive technology teamwork them acquired make their different job reducing productivity errors employees\n"
          ]
        }
      ]
    }
  ]
}