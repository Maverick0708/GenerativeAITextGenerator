{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The below code is to train an LSTM model to generate new text. The application has effectively 2 parts, encoder and decoder. I use a bidirectional LSTM and a unidirectional LSTM for encoder and 2 dense layers as decoder."
      ],
      "metadata": {
        "id": "JgsR4NQqhdpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a tokenized corpus\n",
        "\n",
        "The first step is too read a textfile for creating a corpus. A corpus is a large and structured set of texts used for natural language processing (NLP) tasks like text mining, sentiment analysis, machine translation,etc. I will use this corpus to train my model. The FitOnTexts function of tokenizer is used  to generate a word index dictionary that maps each unique word in the provided texts to a unique integer. This mapping will be used to convert string input into numerical format to be used as input to machine learning models. Machine Learning models only understands numbers"
      ],
      "metadata": {
        "id": "5Q30oKvPiW0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tWUL6VJrhbAx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "text = open(\"employee.txt\", encoding=\"utf-8\").read()\n",
        "corpus = text.lower().splitlines()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "wordDict = len(tokenizer.word_index)+1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating n-gram sequences\n",
        "\n",
        "We need to convert the text input into numericals.TextToSequences is used for this purpose. It converts list of strings into a list of sequences/integers. Each word in the texts is replaced by its corresponding integer index as determined by the fit_on_texts method. Next we make input sequences. Each line needs to be converted to n-gram sequences. Last word in each n-gram sequence will be treated as label and all the words preceding it as inputs. For example, The sentence 'Shivam loves building large language models' should be converted to [\"Shivam\", \"Shivam loves\", \"Shivam loves building\",\"Shivam loves building large\",\"Shivam loves building large language\",\"Shivam loves building large language models\"]. Though for training purposes, we will ignore n-gram sequences with length 1 as they can't have both input and label"
      ],
      "metadata": {
        "id": "lZZe_HK-k8yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngramSequencesList = []\n",
        "for line in corpus:\n",
        "    listOfTokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    #print(token_list)\n",
        "    for i in range(1, len(listOfTokens)):\n",
        "        ngramSequence = listOfTokens[:i+1]\n",
        "        ngramSequencesList.append(ngramSequence)\n",
        "sequenceMaxLength = max([len(i) for i in ngramSequencesList])\n"
      ],
      "metadata": {
        "id": "Xl8ARco9k9Ow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "\n",
        "The n-fram sequences to be used as input must be of same size, so padding is done"
      ],
      "metadata": {
        "id": "plcaLa1wo46w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "ngramSequencesList = np.array(pad_sequences(ngramSequencesList, maxlen=sequenceMaxLength, padding='pre'))"
      ],
      "metadata": {
        "id": "1GCA5zeKo5SN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seperating input from labels and one hot encoding\n",
        "\n",
        "The input and labels need to be seperated for everu n-gram sequence. Labels need to be one hot encoded to remove any relationship between the labels"
      ],
      "metadata": {
        "id": "GTovCs0vpRvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.utils as keras_utils\n",
        "\n",
        "input, label = ngramSequencesList[:,:-1],ngramSequencesList[:,-1]\n",
        "label = keras_utils.to_categorical(label, num_classes=wordDict)"
      ],
      "metadata": {
        "id": "43htE1KwpSQO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model\n",
        "\n",
        "Model contains 2 parts, encoder and decoder\n",
        "\n",
        "## Encoder\n",
        "\n",
        "Encoder consists of an Embedding layer which converts each individual token(represented in numerical form) into dense embedded vectors, each dimensions size mentioned in second parameter(in this case 100). This ensures that the semantics and contexts of the words is captured and the size of the token does not grow quadratically with the size of the corpus.\n",
        "\n",
        " A bidirectional LSTM layers is added. LSTM input shape must be compatable with embedding outsput shape. Since LSTM is not the first layer, keras will take care of this implicitly and we don't need to mention input shape explicitly. return_sequence=True is added to return the full sequence of outputs for another LSTM stack\n",
        "\n",
        " A dropout layer is added to avoid overfitting\n",
        "\n",
        " a unidirectional LSTM is added as the last layer. The output is feeded into the decoder\n",
        "\n",
        "# Decoder\n",
        "\n",
        "Decoder has 2 dense layer. First dense layer is used to recieve input from encoder and relu function is applied. Regulizer is used to avoid overfitting\n",
        "\n",
        "Last sende layer is used as the output layer for the predicted word. Softmax is used in this layer to chose the word with the highest probability\n",
        "\n",
        "Optimizer used is ADAM"
      ],
      "metadata": {
        "id": "zRVVH9cOqlD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#------------------Encoder------------------------------------\n",
        "model.add(Embedding(wordDict, 100, input_length=sequenceMaxLength-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(150))\n",
        "#-------------------Decoder----------------------------------------------\n",
        "model.add(Dense(wordDict, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Dense(wordDict, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "mKvtX_VRqnNY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "Here the model is simply trained"
      ],
      "metadata": {
        "id": "Ro3TBm0ivjuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(input, label, epochs=50, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD3g_EI-viYe",
        "outputId": "f5abf9fe-7c76-473a-dcc2-ae5d809b3081"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "130/130 [==============================] - 82s 572ms/step - loss: 5.8523 - accuracy: 0.0366\n",
            "Epoch 2/50\n",
            "130/130 [==============================] - 73s 564ms/step - loss: 5.4364 - accuracy: 0.0426\n",
            "Epoch 3/50\n",
            "130/130 [==============================] - 76s 582ms/step - loss: 5.2935 - accuracy: 0.0582\n",
            "Epoch 4/50\n",
            "130/130 [==============================] - 72s 558ms/step - loss: 5.1637 - accuracy: 0.0753\n",
            "Epoch 5/50\n",
            "130/130 [==============================] - 75s 576ms/step - loss: 5.0356 - accuracy: 0.0835\n",
            "Epoch 6/50\n",
            "130/130 [==============================] - 75s 571ms/step - loss: 4.8841 - accuracy: 0.1052\n",
            "Epoch 7/50\n",
            "130/130 [==============================] - 74s 570ms/step - loss: 4.7297 - accuracy: 0.1170\n",
            "Epoch 8/50\n",
            "130/130 [==============================] - 75s 580ms/step - loss: 4.5602 - accuracy: 0.1379\n",
            "Epoch 9/50\n",
            "130/130 [==============================] - 72s 556ms/step - loss: 4.4118 - accuracy: 0.1507\n",
            "Epoch 10/50\n",
            "130/130 [==============================] - 73s 565ms/step - loss: 4.2581 - accuracy: 0.1733\n",
            "Epoch 11/50\n",
            "130/130 [==============================] - 76s 581ms/step - loss: 4.1250 - accuracy: 0.1870\n",
            "Epoch 12/50\n",
            "130/130 [==============================] - 77s 595ms/step - loss: 3.9862 - accuracy: 0.2014\n",
            "Epoch 13/50\n",
            "130/130 [==============================] - 76s 584ms/step - loss: 3.8615 - accuracy: 0.2103\n",
            "Epoch 14/50\n",
            "130/130 [==============================] - 74s 572ms/step - loss: 3.7216 - accuracy: 0.2202\n",
            "Epoch 15/50\n",
            "130/130 [==============================] - 76s 579ms/step - loss: 3.5886 - accuracy: 0.2325\n",
            "Epoch 16/50\n",
            "130/130 [==============================] - 71s 550ms/step - loss: 3.4849 - accuracy: 0.2455\n",
            "Epoch 17/50\n",
            "130/130 [==============================] - 75s 580ms/step - loss: 3.3738 - accuracy: 0.2542\n",
            "Epoch 18/50\n",
            "130/130 [==============================] - 74s 572ms/step - loss: 3.2307 - accuracy: 0.2729\n",
            "Epoch 19/50\n",
            "130/130 [==============================] - 74s 568ms/step - loss: 3.1083 - accuracy: 0.2842\n",
            "Epoch 20/50\n",
            "130/130 [==============================] - 73s 556ms/step - loss: 3.0078 - accuracy: 0.2994\n",
            "Epoch 21/50\n",
            "130/130 [==============================] - 75s 577ms/step - loss: 2.8926 - accuracy: 0.3107\n",
            "Epoch 22/50\n",
            "130/130 [==============================] - 75s 570ms/step - loss: 2.8111 - accuracy: 0.3148\n",
            "Epoch 23/50\n",
            "130/130 [==============================] - 74s 573ms/step - loss: 2.6765 - accuracy: 0.3396\n",
            "Epoch 24/50\n",
            "130/130 [==============================] - 75s 579ms/step - loss: 2.5555 - accuracy: 0.3562\n",
            "Epoch 25/50\n",
            "130/130 [==============================] - 74s 572ms/step - loss: 2.4393 - accuracy: 0.3747\n",
            "Epoch 26/50\n",
            "130/130 [==============================] - 74s 569ms/step - loss: 2.3180 - accuracy: 0.3993\n",
            "Epoch 27/50\n",
            "130/130 [==============================] - 73s 562ms/step - loss: 2.1969 - accuracy: 0.4221\n",
            "Epoch 28/50\n",
            "130/130 [==============================] - 74s 573ms/step - loss: 2.0901 - accuracy: 0.4330\n",
            "Epoch 29/50\n",
            "130/130 [==============================] - 76s 583ms/step - loss: 1.9765 - accuracy: 0.4633\n",
            "Epoch 30/50\n",
            "130/130 [==============================] - 74s 569ms/step - loss: 1.8674 - accuracy: 0.4864\n",
            "Epoch 31/50\n",
            "130/130 [==============================] - 76s 586ms/step - loss: 1.7571 - accuracy: 0.5112\n",
            "Epoch 32/50\n",
            "130/130 [==============================] - 72s 553ms/step - loss: 1.6600 - accuracy: 0.5389\n",
            "Epoch 33/50\n",
            "130/130 [==============================] - 74s 571ms/step - loss: 1.6567 - accuracy: 0.5389\n",
            "Epoch 34/50\n",
            "130/130 [==============================] - 76s 586ms/step - loss: 1.5025 - accuracy: 0.5889\n",
            "Epoch 35/50\n",
            "130/130 [==============================] - 77s 593ms/step - loss: 1.3796 - accuracy: 0.6161\n",
            "Epoch 36/50\n",
            "130/130 [==============================] - 77s 592ms/step - loss: 1.2531 - accuracy: 0.6575\n",
            "Epoch 37/50\n",
            "130/130 [==============================] - 78s 597ms/step - loss: 1.1254 - accuracy: 0.7006\n",
            "Epoch 38/50\n",
            "130/130 [==============================] - 76s 584ms/step - loss: 1.0524 - accuracy: 0.7155\n",
            "Epoch 39/50\n",
            "130/130 [==============================] - 74s 573ms/step - loss: 0.9915 - accuracy: 0.7379\n",
            "Epoch 40/50\n",
            "130/130 [==============================] - 72s 556ms/step - loss: 1.0107 - accuracy: 0.7302\n",
            "Epoch 41/50\n",
            "130/130 [==============================] - 77s 591ms/step - loss: 1.0008 - accuracy: 0.7271\n",
            "Epoch 42/50\n",
            "130/130 [==============================] - 72s 556ms/step - loss: 0.7893 - accuracy: 0.7964\n",
            "Epoch 43/50\n",
            "130/130 [==============================] - 73s 559ms/step - loss: 0.6862 - accuracy: 0.8397\n",
            "Epoch 44/50\n",
            "130/130 [==============================] - 77s 591ms/step - loss: 0.6076 - accuracy: 0.8643\n",
            "Epoch 45/50\n",
            "130/130 [==============================] - 74s 571ms/step - loss: 0.5518 - accuracy: 0.8797\n",
            "Epoch 46/50\n",
            "130/130 [==============================] - 76s 586ms/step - loss: 0.5308 - accuracy: 0.8869\n",
            "Epoch 47/50\n",
            "130/130 [==============================] - 75s 579ms/step - loss: 0.4943 - accuracy: 0.8953\n",
            "Epoch 48/50\n",
            "130/130 [==============================] - 75s 572ms/step - loss: 0.4557 - accuracy: 0.9083\n",
            "Epoch 49/50\n",
            "130/130 [==============================] - 72s 558ms/step - loss: 0.4408 - accuracy: 0.9134\n",
            "Epoch 50/50\n",
            "130/130 [==============================] - 74s 573ms/step - loss: 0.4139 - accuracy: 0.9218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "A prompt is used for the model to use as input for predicting next words. I predict upto 100 words here(Prompt size + 100 words)"
      ],
      "metadata": {
        "id": "SAPxu6CTviHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me Something about employee training\"\n",
        "prectionWodCOunt = 100\n",
        "\n",
        "for _ in range(prectionWodCOunt):\n",
        "    listOfTokens = tokenizer.texts_to_sequences([prompt])[0]\n",
        "    listOfTokens = pad_sequences([listOfTokens], maxlen=sequenceMaxLength-1, padding='pre')\n",
        "    predictedProbabilities = model.predict(listOfTokens, verbose=0)\n",
        "    predictedClass = np.argmax(predictedProbabilities)\n",
        "    outWord = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predictedClass:\n",
        "            outWord = word\n",
        "            break\n",
        "    prompt += \" \" + outWord\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt0fTWAxybi6",
        "outputId": "5bdc0918-d504-46ff-9616-89279c26b659"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me Something about employee training skills results from employees patient in working for one organization for a considerable amount of time it enables employees to appreciate the process of organizational growth they become patient to grow with it employee training enables workers to appreciate that the more one remains in an organization the less likely that he or she will turn over that are occasionally trained on various topics and issues gain the required skills for promotion and reward such employees will always be waiting for the next training programs in the employees who worked as office assistants during the manual era however those who\n"
          ]
        }
      ]
    }
  ]
}